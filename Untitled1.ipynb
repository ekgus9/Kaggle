{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkxh06zq2nEsqHdRVZeOa0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY9UZJJqfFqY"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install seqeval"
      ],
      "metadata": {
        "id": "hjqhglh_fvN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import shape_list, BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "R6iwlaDrfxEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jatBP8yZkWn6Kg6mjN7nWLnYVwXE_sY_' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1jatBP8yZkWn6Kg6mjN7nWLnYVwXE_sY_\" -O ner_train_data.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YVYShKCtWfigXBOb5ie7s6QmA-dHnjt3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1YVYShKCtWfigXBOb5ie7s6QmA-dHnjt3\" -O ner_test_data.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_DPfdY1Q5Xt2md7QVbKcQLRDYHm3qgVQ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1_DPfdY1Q5Xt2md7QVbKcQLRDYHm3qgVQ\" -O ner_label.txt && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "EnBnNXOpfyyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ner_df = pd.read_csv(\"ner_train_data.csv\")\n",
        "train_ner_df.head()"
      ],
      "metadata": {
        "id": "qEIAmrg4f5vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ner_df = pd.read_csv(\"ner_test_data.csv\")\n",
        "test_ner_df.head()"
      ],
      "metadata": {
        "id": "5q-PBiy-f_Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"학습 데이터 샘플 개수 :\", len(train_ner_df))\n",
        "print(\"테스트 데이터 샘플 개수 :\", len(test_ner_df))"
      ],
      "metadata": {
        "id": "wGcudVsAgA0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_sentence = [sent.split() for sent in train_ner_df['Sentence'].values]\n",
        "test_data_sentence = [sent.split() for sent in test_ner_df['Sentence'].values]\n",
        "train_data_label = [tag.split() for tag in train_ner_df['Tag'].values]\n",
        "test_data_label = [tag.split() for tag in test_ner_df['Tag'].values]"
      ],
      "metadata": {
        "id": "IDiw3CblgCi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_to_index = {tag: index for index, tag in enumerate(labels)}\n",
        "index_to_tag = {index: tag for index, tag in enumerate(labels)}\n",
        "\n",
        "tag_size = len(tag_to_index)"
      ],
      "metadata": {
        "id": "Qu4BOBMggMJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "DMFHo8g9gV6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer,\n",
        "                                 pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
        "\n",
        "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
        "        tokens = []\n",
        "        labels_ids = []\n",
        "        for one_word, label_token in zip(example, label):\n",
        "            subword_tokens = tokenizer.tokenize(one_word)\n",
        "            tokens.extend(subword_tokens)\n",
        "            labels_ids.extend([tag_to_index[label_token]]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
        "\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            labels_ids = labels_ids[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        tokens += [sep_token]\n",
        "        labels_ids += [pad_token_id_for_label]\n",
        "        tokens = [cls_token] + tokens\n",
        "        labels_ids = [pad_token_id_for_label] + labels_ids\n",
        "\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_id)\n",
        "        padding_count = max_seq_len - len(input_id)\n",
        "\n",
        "        input_id = input_id + ([pad_token_id] * padding_count)\n",
        "        attention_mask = attention_mask + ([0] * padding_count)\n",
        "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
        "        label = labels_ids + ([pad_token_id_for_label] * padding_count)\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "        assert len(label) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label), max_seq_len)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        data_labels.append(label)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), data_labels"
      ],
      "metadata": {
        "id": "OnRm_3ikgYwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = convert_examples_to_features(train_data_sentence, train_data_label, max_seq_len=128, tokenizer=tokenizer)\n",
        "\n",
        "X_test, y_test = convert_examples_to_features(test_data_sentence, test_data_label, max_seq_len=128, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "HYRqnQYUgbyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForTokenClassification\n",
        "# TPU 작동을 위한 코드\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  model = TFBertForTokenClassification.from_pretrained(\"klue/bert-base\", num_labels=tag_size, from_pt=True)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "  model.compile(optimizer=optimizer, loss=model.compute_loss)"
      ],
      "metadata": {
        "id": "htWhIYGRggGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class F1score(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X_test, y_test):\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def sequences_to_tags(self, label_ids, pred_ids):\n",
        "      label_list = []\n",
        "      pred_list = []\n",
        "\n",
        "      for i in range(0, len(label_ids)):\n",
        "        label_tag = []\n",
        "        pred_tag = []\n",
        "\n",
        "        for label_index, pred_index in zip(label_ids[i], pred_ids[i]):\n",
        "          if label_index != -100:\n",
        "            label_tag.append(index_to_tag[label_index])\n",
        "            pred_tag.append(index_to_tag[pred_index])\n",
        "        \n",
        "        label_list.append(label_tag)\n",
        "        pred_list.append(pred_tag)\n",
        "\n",
        "      return label_list, pred_list\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "      y_predicted = self.model.predict(self.X_test)\n",
        "      y_predicted = np.argmax(y_predicted.logits, axis = 2)\n",
        "\n",
        "      label_list, pred_list = self.sequences_to_tags(self.y_test, y_predicted)\n",
        "\n",
        "      score = f1_score(label_list, pred_list, suffix=True)\n",
        "      print(' - f1: {:04.2f}'.format(score * 100))\n",
        "      print(classification_report(label_list, pred_list, suffix=True))"
      ],
      "metadata": {
        "id": "ZPR7ncplgph_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score_report = F1score(X_test, y_test)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train, epochs=3, batch_size=32,\n",
        "    callbacks = [f1_score_report]\n",
        ")\n"
      ],
      "metadata": {
        "id": "UBN2HyVygrTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features_for_prediction(examples, max_seq_len, tokenizer,\n",
        "                                 pad_token_id_for_segment=0, pad_token_id_for_label=-100):\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, label_masks = [], [], [], []\n",
        "\n",
        "    for example in tqdm(examples):\n",
        "        tokens = []\n",
        "        label_mask = []\n",
        "        for one_word in example:\n",
        "            subword_tokens = tokenizer.tokenize(one_word)\n",
        "            tokens.extend(subword_tokens)\n",
        "            label_mask.extend([0]+ [pad_token_id_for_label] * (len(subword_tokens) - 1))\n",
        "\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            label_mask = label_mask[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        tokens += [sep_token]\n",
        "        label_mask += [pad_token_id_for_label]\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_mask = [pad_token_id_for_label] + label_mask\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_id)\n",
        "        padding_count = max_seq_len - len(input_id)\n",
        "        input_id = input_id + ([pad_token_id] * padding_count)\n",
        "        attention_mask = attention_mask + ([0] * padding_count)\n",
        "        token_type_id = [pad_token_id_for_segment] * max_seq_len\n",
        "        label_mask = label_mask + ([pad_token_id_for_label] * padding_count)\n",
        "\n",
        "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
        "        assert len(label_mask) == max_seq_len, \"Error with labels length {} vs {}\".format(len(label_mask), max_seq_len)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        label_masks.append(label_mask)\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype=int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
        "    label_masks = np.asarray(label_masks, dtype=np.int32)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids), label_masks"
      ],
      "metadata": {
        "id": "lQWgFiZtgxr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pred, label_masks = convert_examples_to_features_for_prediction(test_data_sentence[:5], max_seq_len=128, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "DVCU5mCPg9-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ner_prediction(examples, max_seq_len, tokenizer):\n",
        "  examples = [sent.split() for sent in examples]\n",
        "  X_pred, label_masks = convert_examples_to_features_for_prediction(examples, max_seq_len=128, tokenizer=tokenizer)\n",
        "  y_predicted = model.predict(X_pred)\n",
        "  y_predicted = np.argmax(y_predicted.logits, axis = 2)\n",
        "\n",
        "  pred_list = []\n",
        "  result_list = []\n",
        "\n",
        "  for i in range(0, len(label_masks)):\n",
        "    pred_tag = []\n",
        "    for label_index, pred_index in zip(label_masks[i], y_predicted[i]):\n",
        "      if label_index != -100:\n",
        "        pred_tag.append(index_to_tag[pred_index])\n",
        "\n",
        "    pred_list.append(pred_tag)\n",
        "\n",
        "  for example, pred in zip(examples, pred_list):\n",
        "    one_sample_result = []\n",
        "    for one_word, label_token in zip(example, pred):\n",
        "      one_sample_result.append((one_word, label_token))\n",
        "    result_list.append(one_sample_result)\n",
        "\n",
        "  return result_list"
      ],
      "metadata": {
        "id": "o6YPX4f3g_V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = '오리온스는 리그 최정상급 포인트가드 김동훈을 앞세우는 빠른 공수전환이 돋보이는 팀이다'\n",
        "sent2 = '하이신사에 속한 섬들도 위로 솟아 있는데 타인은 살고 있어요'\n",
        "test_samples = [sent1, sent2]"
      ],
      "metadata": {
        "id": "ehLx9MCNhBuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_list = ner_prediction(test_samples, max_seq_len=128, tokenizer=tokenizer)\n",
        "\n",
        "result_list"
      ],
      "metadata": {
        "id": "EERF_e4qhDZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bilstm"
      ],
      "metadata": {
        "id": "UCjnneWInI96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Q-2qPcspnJKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 4000\n",
        "src_tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')\n",
        "src_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "tar_tokenizer = Tokenizer()\n",
        "tar_tokenizer.fit_on_texts(ner_tags)"
      ],
      "metadata": {
        "id": "WWv86Ic5p82Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_size = len(tar_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "sndjffuRqB7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = src_tokenizer.texts_to_sequences(sentences)\n",
        "y_train = tar_tokenizer.texts_to_sequences(ner_tags)\n",
        "\n",
        "index_to_word = src_tokenizer.index_word\n",
        "index_to_ner = tar_tokenizer.index_word"
      ],
      "metadata": {
        "id": "jTZYdeg4qCaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 70\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=max_len)\n",
        "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
      ],
      "metadata": {
        "id": "ZhZYSdbUqIYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)"
      ],
      "metadata": {
        "id": "BMzPfsTcqKAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = to_categorical(y_test, num_classes=tag_size)"
      ],
      "metadata": {
        "id": "uz3DQRSzqLV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=8, validation_data=(X_test, y_test))\n",
        "\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
      ],
      "metadata": {
        "id": "g-22eeLfqMu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 10 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "\n",
        "# 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "y_predicted = model.predict(np.array([X_test[i]]))\n",
        "\n",
        "# 확률 벡터를 정수 레이블로 변경.\n",
        "y_predicted = np.argmax(y_predicted, axis=-1)\n",
        "\n",
        "# 원-핫 벡터를 정수 인코딩으로 변경.\n",
        "labels = np.argmax(y_test[i], -1)\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag].upper(), index_to_ner[pred].upper()))"
      ],
      "metadata": {
        "id": "rb2Rw7owqS0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "def sequences_to_tag(sequences):\n",
        "    result = []\n",
        "    # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
        "    for sequence in sequences:\n",
        "        word_sequence = []\n",
        "        # 시퀀스로부터 확률 벡터 또는 원-핫 벡터를 하나씩 꺼낸다.\n",
        "        for pred in sequence:\n",
        "            # 정수로 변환. 예를 들어 pred가 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
        "            pred_index = np.argmax(pred)            \n",
        "            # index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경.\n",
        "            word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "        result.append(word_sequence)\n",
        "    return result\n",
        "\n",
        "y_predicted = model.predict([X_test])\n",
        "pred_tags = sequences_to_tag(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "PkihQu8iqht6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bilstm-crf"
      ],
      "metadata": {
        "id": "dEMi9qkTqr1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-crf"
      ],
      "metadata": {
        "id": "lio5oJi4quw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Bidirectional, TimeDistributed, Embedding, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras_crf import CRFModel\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 64\n",
        "dropout_ratio = 0.3\n",
        "\n",
        "sequence_input = Input(shape=(max_len,),dtype=tf.int32, name='sequence_input')\n",
        "\n",
        "model_embedding = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=max_len)(sequence_input)\n",
        "\n",
        "model_bilstm = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(model_embedding)\n",
        "\n",
        "model_dropout = TimeDistributed(Dropout(dropout_ratio))(model_bilstm)\n",
        "\n",
        "model_dense = TimeDistributed(Dense(tag_size, activation='relu'))(model_dropout)\n",
        "\n",
        "base = Model(inputs=sequence_input, outputs=model_dense)\n",
        "model = CRFModel(base, tag_size)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')"
      ],
      "metadata": {
        "id": "kmuy44WCq4oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])"
      ],
      "metadata": {
        "id": "yP4Ya6bvq5cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('bilstm_crf/cp.ckpt')\n",
        "\n",
        "i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.\n",
        "y_predicted = model.predict(np.array([X_test[i]]))[0] # 입력한 테스트용 샘플에 대해서 예측 y를 리턴\n",
        "labels = np.argmax(y_test[i], -1) # 원-핫 인코딩을 다시 정수 인코딩으로 변경.\n",
        "\n",
        "print(\"{:15}|{:5}|{}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
        "print(35 * \"-\")\n",
        "\n",
        "for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):\n",
        "    if word != 0: # PAD값은 제외함.\n",
        "        print(\"{:17}: {:7} {}\".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))"
      ],
      "metadata": {
        "id": "YutRHWRfq6wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequences_to_tag_for_crf(sequences): \n",
        "    result = []\n",
        "    # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
        "    for sequence in sequences: \n",
        "        word_sequence = []\n",
        "        # 시퀀스로부터 예측 정수 레이블을 하나씩 꺼낸다.\n",
        "        for pred_index in sequence:\n",
        "            # index_to_ner을 사용하여 정수를 태깅 정보로 변환. 'PAD'는 'O'로 변경.\n",
        "            word_sequence.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))\n",
        "        result.append(word_sequence)\n",
        "    return result\n",
        "\n",
        "pred_tags = sequences_to_tag_for_crf(y_predicted)\n",
        "test_tags = sequences_to_tag(y_test)\n",
        "\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))\n",
        "print(classification_report(test_tags, pred_tags))"
      ],
      "metadata": {
        "id": "Wsq0NvGurAy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}